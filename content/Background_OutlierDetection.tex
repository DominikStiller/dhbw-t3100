The quality of the estimate produces by the estimation algorithm is limited by the quality of its input data, especially if the algorithm is not robust. In practice, sensor data can contain anomalous sensor samples. While these samples might actually reflect reality, they usually stem from errors and failures during measurement or transmission. In this section, the term \textit{failure} will refer to any bad samples that should be discarded, not surprising but correct samples which can occur in random processes. The challenge is to discern these two and minimize the number of false positives (leads to discarding good measurements) and false negatives (degrades estimation accuracy), which improves the quality of the state estimate and can be crucial in safety-critical applications. This section presents common failure types and methods to detect them.


\subsection{Failure Types}\label{sec:failure-types}
Failures can be the result of a variety of issues. For example, heavy vibration and shocks might cause acceleration spikes and an optical cross-correlation sensor might look at a featureless surface, so the measured velocity drops to zero. In cabled transmission, a physical connection might be defect, while in radio transmission, the signal might drop out temporarily or there might be electromagnetic interference. Possible causes can vary a lot between environments, but any highly dynamic environment with many mechanical and electric parts such as an electric race car is likely to produce failures at some point.

Failures can be classified into four broad categories~\cites[p.~19]{Kabzan.2019}[p.~165~ff.]{Himmelblau.1994}, shown in figure~\ref{fig:failure-types}:
\begin{itemize}
\item Outlier/intermittent spike: a transient deviation from previous samples
\item Level shift: a transient variation of the mean value
\item Drift/spurious trends: a slow variation of the mean value over time
\item Null/signal dropout: not receiving input or zero
\end{itemize}
Drift often occurs as the result of integrating a biased signal, e.g., caused by low frequency noise or temperature changes. The true mean does not change, but the accumulating error results in an observed trending mean. While there are other cases of poor data quality such as clipping and excessive noise, they rather stem from calibration problems and hardware errors, so they are not regarded further.

\begin{figure}[t]
	\begin{subfigure}[c]{0.5\textwidth}
		\centering
		\includegraphics[width=0.95\textwidth]{plot_outlier_spike}%
		\subcaption{Outlier}
		\label{fig:failure-type-outlier}
	\end{subfigure}
	\begin{subfigure}[c]{0.5\textwidth}
		\centering
		\includegraphics[width=0.95\textwidth]{plot_outlier_levelshift}%
		\subcaption{Level shift}
		\label{fig:failure-type-levelshift}
	\end{subfigure}
	\begin{subfigure}[c]{0.5\textwidth}
		\centering
		\includegraphics[width=0.95\textwidth]{plot_outlier_drift}%
		\subcaption{Drift}
		\label{fig:failure-type-drift}
	\end{subfigure}
	\begin{subfigure}[c]{0.5\textwidth}
		\centering
		\includegraphics[width=0.95\textwidth]{plot_outlier_null}%
		\subcaption{Null}
		\label{fig:failure-type-null}
	\end{subfigure}
	\caption{Failure types}
	\label{fig:failure-types}
\end{figure}

Failures can be temporary or persistent. While outliers are inherently temporary, failures of the other three categories might or might not return to normal. For example, a \gls{gnss} signal can drop out because the sensor has no satellite connectivity for a couple of seconds but then recovers (as in figure \ref{fig:failure-type-null}), while another sensor can be permanently damaged and will not recover (as in figure \ref{fig:failure-type-levelshift}). Persistent but not transient failures are hard to detect, and especially spurious trends cannot easily be distinguished from real trends by their nature. This motivates the need for two separate methods, an approach that is also employed by~\cite{Kabzan.2019}:
\begin{itemize}
\item A simple but strict method for transient failures
\item A complex but lenient method for persistent failures
\end{itemize}
With this dual approach, false negatives for easy-to-detect transient failures can be minimized while false positives for hard-to-discern persistent failures can be avoided. Note that the choice of detection thresholds influences the strictness and therefore sensitivity to a high degree.


\subsection{Rudimentary Methods}
Arguably the simplest method to detect sensor failures is the range check, shown in equation \ref{eq:failure-range}. A plausible interval $[\tau_{min}, \tau_{max}]$ of values is defined in advance, and if the measurement $z$ is outside of that range, it is marked as failure. For example, a speed of \SI{200}{\meter\per\second} is highly unlikely even for a race car, and high negative speeds are implausible as well.
\begin{equation}\label{eq:failure-range}%
\tau_{min} < z < \tau_{max}%
\end{equation}

Another rudimentary method for detecting outlier is the difference of consecutive samples $z_{t-1}, z_t$, shown in equation \ref{eq:failure-maxchangerate}. When the difference is higher than the maximum plausible change rate $\tau$, the current measurement is marked as failure.
\begin{equation}\label{eq:failure-maxchangerate}%
\abs{z_t - z_{t-1}} < \tau%
\end{equation}

Performing a sanity check is a good first approach to detect gross failures. However, the thresholds should be set rather high to avoid false positives, e.g., when noise creates large differences.


\subsection{Statistical Methods}\label{sec:failure-statisticalmethods}
Methods based on the statistical properties of a signal can provide more granular checks than the previous rudimentary methods. One of the simplest robust statistics is the median, i.e. the center value when sorting a list of values, which is used in the method shown in equation \ref{eq:failure-median}~\cite[p.~142]{Basu.2007}. An expected value $\tilde{z}_t$ is calculated using both the median of the last $k$ samples and the median of the differences of the last $k+1$ samples. If the difference between the expected and the actual value exceeds the threshold, the current sample is marked as failure. The window size $k$ influences the robustness but also the detection delay, and should therefore be chosen carefully. Note that, once the failure persists for more than $k$ samples, following measurements will be regarded as valid.
\begin{equation}\label{eq:failure-median}%
\abs*{z_t - \underbrace{median(z_{t-k}, \ldots, z_{t-1}) + k \cdot median(z_{t-k} - z_{t-k-1}, \ldots, z_{t-1} - z_{t-2})}_\textrm{$\tilde{z}_t$}} < \tau%
\end{equation}

If an \gls{ekf} is used, a chi-squared test can detect anomalies based on the residuals, as shown in equation \ref{eq:failure-chisquared}~\cites[p.~4292]{Hausman.2016}[p.~2050~f.]{Valls.2018}. Since the residual $y$ is Gaussian distributed with the innovation covariance matrix $P_{yy}$, the \gls{nssr} should be distributed according to a chi-squared distribution $\chi^2$ with as many degrees of freedom as there are measurements. A failure is detected when the chi-squared test at a significance level of $\tau$ fails.
\begin{subequations}\label{eq:failure-chisquared}
\begin{alignat}{2}%
y &= z - h(\hat{x}^-) \\%
P_{yy} &= H P^- H^T + R \\%
y^T P_{yy}^{-1} y &< \chi^2(\tau)%
\end{alignat}
\end{subequations}

Drift detection becomes easier when multiple sensors measure the same variable and can be compared. A simple variance-based approach based on this idea is shown in equation \ref{eq:failure-variance}~\cite[p.~20]{Kabzan.2019}. For each time step, the summed variance of $n$ sensors is calculated, based on the mean $\mu_z$ of their measurements at that instant. Once the threshold $\tau \in (0, 1)$ is exceeded, the measurement with the highest contribution to the summed variance is marked as failure.
\begin{equation}\label{eq:failure-variance}%
\sum_{i=1}^n (z_i - \mu_z)^2 < \tau%
\end{equation}
The last approach we will regard can be used for outlier and drift detection, but requires $n>2$ sensors for a variable like the previous method. Multiple instances of the same \gls{ekf}, collectively called a bank of $n$ \gls{ekf}s, are executed in parallel. However, in the $i$-th filter, the measurement of the $i$-th sensor is disabled. In the event that a failure in the $i$-th measurement occurs, all filters except the one that does not incorporate the faulty measurement will show a high residual. \\ Equation \ref{eq:failure-ekfbank}~\cite[p.~3]{Kobayashi.2003} shows how the sum of squared residuals\footnote{While the original paper refers to a \textit{weighted} sum of squared residuals, we omitted the weight coefficient in favor of an adjustable threshold.} $\textit{NSSR}^i$, normalized by the diagonal measurement covariance matrix $R^i$, is calculated from the residual $y_i$ of the $i$-th filter. The \gls{nssr} combines all elements of the residual vector into a single metric, enabling easy comparison with a threshold $\tau_i$. This threshold can be chosen empirically, or based on a desired statistical significance of the resulting $\chi^2$ distribution~\cite[p.~3]{Xue.2007}. Note that a more sophisticated approach to fault isolation, i.e. locating the sensor producing failures, is required for the \gls{ekf} bank to work with multiple simultaneous failures.
\begin{subequations}\label{eq:failure-ekfbank}
\begin{alignat}{2}%
y^i &= z^i - h^i(\hat{x}^-) \\%
\underbrace{(y^i)^T (R^i)^{-1} y^i}_\textrm{$\textit{NSSR}^i$} &< \tau_i%
\end{alignat}
\end{subequations}
